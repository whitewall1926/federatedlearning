{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  # to set the python random seed\n",
    "import numpy  # to set the numpy random seed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"thorough-pytorch\",\n",
    "           name=\"wandb_demo\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "config = wandb.config  # config的初始化\n",
    "config.batch_size = 64  \n",
    "config.test_batch_size = 10 \n",
    "config.epochs = 10\n",
    "config.lr = 0.01 \n",
    "config.momentum = 0.1  \n",
    "config.use_cuda = True  \n",
    "config.seed = 2043  \n",
    "config.log_interval = 10 \n",
    "\n",
    "# 设置随机数\n",
    "def set_seed(seed):\n",
    "    random.seed(config.seed)      \n",
    "    torch.manual_seed(config.seed) \n",
    "    numpy.random.seed(config.seed) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for batch_id, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# wandb.log用来记录一些日志(accuracy,loss and epoch), 便于随时查看网路的性能\n",
    "def test(model, device, test_loader, classes):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    example_images = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            example_images.append(wandb.Image(\n",
    "                data[0], caption=\"Pred:{} Truth:{}\".format(classes[pred[0].item()], classes[target[0]])))\n",
    "\n",
    "   # 使用wandb.log 记录你想记录的指标\n",
    "    wandb.log({\n",
    "        \"Examples\": example_images,\n",
    "        \"Test Accuracy\": 100. * correct / len(test_loader.dataset),\n",
    "        \"Test Loss\": test_loss\n",
    "    })\n",
    "\n",
    "wandb.watch_called = False \n",
    "\n",
    "\n",
    "def main():\n",
    "    use_cuda = config.use_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "    # 设置随机数\n",
    "    set_seed(config.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # 数据预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # 加载数据\n",
    "    train_loader = DataLoader(datasets.CIFAR10(\n",
    "        root='dataset',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    ), batch_size=config.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader = DataLoader(datasets.CIFAR10(\n",
    "        root='dataset',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    ), batch_size=config.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    model = resnet18(pretrained=True).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum)\n",
    "\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        train(model, device, train_loader, optimizer)\n",
    "        test(model, device, test_loader, classes)\n",
    "\n",
    "    # 本地和云端模型保存\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    wandb.save('model.pth')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"momentum\": 0.2,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset_id\": \"cats-0192\",\n",
    "}\n",
    "\n",
    "with wandb.init(\n",
    "    project=\"detect-cats\",\n",
    "    notes=\"tweak baseline\",\n",
    "    tags=[\"baseline\", \"paper1\"],\n",
    "    name=\"test f(i) = i + 2\",\n",
    "    config=config,\n",
    ") as run:\n",
    "    print(\"test\", run.entity)\n",
    "    for i in range(100):\n",
    "        run.log({\n",
    "            \"epoch\":i, \n",
    "            'test': i+ 10\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\SAVE\\Code\\CodeForces\\test-fl\\wandb\\run-20250331_213434-7gcyznth</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/whitewall_9-jinan-university/your_project_name/runs/7gcyznth' target=\"_blank\">whole-monkey-1</a></strong> to <a href='https://wandb.ai/whitewall_9-jinan-university/your_project_name' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/whitewall_9-jinan-university/your_project_name' target=\"_blank\">https://wandb.ai/whitewall_9-jinan-university/your_project_name</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/whitewall_9-jinan-university/your_project_name/runs/7gcyznth' target=\"_blank\">https://wandb.ai/whitewall_9-jinan-university/your_project_name/runs/7gcyznth</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>metric</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>metric</td><td>299</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">whole-monkey-1</strong> at: <a href='https://wandb.ai/whitewall_9-jinan-university/your_project_name/runs/7gcyznth' target=\"_blank\">https://wandb.ai/whitewall_9-jinan-university/your_project_name/runs/7gcyznth</a><br> View project at: <a href='https://wandb.ai/whitewall_9-jinan-university/your_project_name' target=\"_blank\">https://wandb.ai/whitewall_9-jinan-university/your_project_name</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250331_213434-7gcyznth\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (2.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "CommError",
     "evalue": "failed to upsert bucket: returned error 400: {\"data\":{\"upsertBucket\":null},\"errors\":[{\"message\":\"Forking runs is not enabled for your project - contact support@wandb.com to enable it.\",\"path\":[\"upsertBucket\"]}]}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m run1\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Fork from the first run at a specific step and log the metric starting from step 200\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m run2 \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myour_project_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfork_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrun1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m?_step=200\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Continue logging in the new run\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# For the first few steps, log the metric as is from run1\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# After step 250, start logging the spikey pattern\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m300\u001b[39m):\n",
      "File \u001b[1;32md:\\INSTALL\\Anaconda\\envs\\dl\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1482\u001b[0m, in \u001b[0;36minit\u001b[1;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[0;32m   1478\u001b[0m     wl\u001b[38;5;241m.\u001b[39m_get_logger()\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[1;32m-> 1482\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n",
      "File \u001b[1;32md:\\INSTALL\\Anaconda\\envs\\dl\\lib\\site-packages\\wandb\\analytics\\sentry.py:156\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[1;34m(self, exc)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32md:\\INSTALL\\Anaconda\\envs\\dl\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1468\u001b[0m, in \u001b[0;36minit\u001b[1;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[0;32m   1465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_settings\u001b[38;5;241m.\u001b[39mx_server_side_derived_summary:\n\u001b[0;32m   1466\u001b[0m         init_telemetry\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mserver_side_derived_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wl:\n",
      "File \u001b[1;32md:\\INSTALL\\Anaconda\\envs\\dl\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:963\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[1;34m(self, settings, config)\u001b[0m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result\u001b[38;5;241m.\u001b[39mrun_result\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;241m:=\u001b[39m ProtobufErrorHandler\u001b[38;5;241m.\u001b[39mto_exception(result\u001b[38;5;241m.\u001b[39mrun_result\u001b[38;5;241m.\u001b[39merror):\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mrun_result\u001b[38;5;241m.\u001b[39mHasField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssertion failed: run_result is missing the run field\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mCommError\u001b[0m: failed to upsert bucket: returned error 400: {\"data\":{\"upsertBucket\":null},\"errors\":[{\"message\":\"Forking runs is not enabled for your project - contact support@wandb.com to enable it.\",\"path\":[\"upsertBucket\"]}]}"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import math\n",
    "\n",
    "# Initialize the first run and log some metrics\n",
    "run1 = wandb.init(project=\"your_project_name\")\n",
    "for i in range(300):\n",
    "    run1.log({\"metric\": i})\n",
    "run1.finish()\n",
    "\n",
    "# Fork from the first run at a specific step and log the metric starting from step 200\n",
    "run2 = wandb.init(\n",
    "    project=\"your_project_name\", fork_from=f\"{run1.id}?_step=200\"\n",
    ")\n",
    "\n",
    "# Continue logging in the new run\n",
    "# For the first few steps, log the metric as is from run1\n",
    "# After step 250, start logging the spikey pattern\n",
    "for i in range(200, 300):\n",
    "    if i < 250:\n",
    "        run2.log({\"metric\": i})  # Continue logging from run1 without spikes\n",
    "    else:\n",
    "        # Introduce the spikey behavior starting from step 250\n",
    "        subtle_spike = i + (2 * math.sin(i / 3.0))  # Apply a subtle spikey pattern\n",
    "        run2.log({\"metric\": subtle_spike})\n",
    "    # Additionally log the new metric at all steps\n",
    "    run2.log({\"additional_metric\": i * 1.1})\n",
    "run2.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
